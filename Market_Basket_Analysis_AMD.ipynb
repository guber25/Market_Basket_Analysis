{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvngheGehM/mvvJDLGEqBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guber25/Market_Basket_Analysis/blob/main/Market_Basket_Analysis_AMD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Algorithms for Massive Data**\n",
        "## Market Basket Analysis using [LinkedIn job skills](https://www.kaggle.com/datasets/asaniczka/1-3m-linkedin-jobs-and-skills-2024) dataset\n",
        "\n",
        "### Author: Guglielmo Berzano\n",
        "#### email: guglielmo.berzano@studenti.unimi.it\n",
        "#### Academic year: 2023-2024\n",
        "\n",
        "Installing pyspark and importing the libraries"
      ],
      "metadata": {
        "id": "IjH6tdlwmeEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "import pyspark\n",
        "\n",
        "from itertools import combinations\n",
        "import os\n",
        "\n",
        "import timeit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXCu13iiqA51",
        "outputId": "49632fb9-335b-45b7-fb04-9e3ba3be4fd1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=fd54d67b7c11b668bf9e03d0b3494a30f44300595c582bf9c9802270e9197c33\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSwkCLwAhqTI",
        "outputId": "c938062c-2310-48c0-fc71-b98fb6ece8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1-3m-linkedin-jobs-and-skills-2024.zip to /content\n",
            "100% 1.88G/1.88G [00:19<00:00, 158MB/s]\n",
            "100% 1.88G/1.88G [00:19<00:00, 102MB/s]\n",
            "Archive:  1-3m-linkedin-jobs-and-skills-2024.zip\n",
            "  inflating: data/job_skills.csv     \n",
            "  inflating: data/job_summary.csv    \n",
            "  inflating: data/linkedin_job_postings.csv  \n"
          ]
        }
      ],
      "source": [
        "#logging in into Kaggle and downloading the dataset\n",
        "\n",
        "#kaggle enviroment\n",
        "os.environ['KAGGLE_USERNAME'] = \"xxxxxx\"\n",
        "os.environ['KAGGLE_KEY'] = \"xxxxxx\"\n",
        "\n",
        "#dataset download\n",
        "!kaggle datasets download -d asaniczka/1-3m-linkedin-jobs-and-skills-2024\n",
        "\n",
        "#dataset unzip\n",
        "!unzip 1-3m-linkedin-jobs-and-skills-2024.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Apriori\").getOrCreate()\n",
        "\n",
        "#importing the dataset into spark\n",
        "\n",
        "raw_df = spark.read.csv(\"/content/data/job_skills.csv\", header=True,sep=\",\")\n",
        "\n",
        "raw_df.show()"
      ],
      "metadata": {
        "id": "rcq_ViKvoHZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba91a20-1ff7-4b30-88ed-331c74f828f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|            job_link|          job_skills|\n",
            "+--------------------+--------------------+\n",
            "|https://www.linke...|Building Custodia...|\n",
            "|https://www.linke...|Customer service,...|\n",
            "|https://www.linke...|Applied Behavior ...|\n",
            "|https://www.linke...|Electrical Engine...|\n",
            "|https://www.linke...|Electrical Assemb...|\n",
            "|https://www.linke...|Access Control, V...|\n",
            "|https://www.linke...|Consultation, Sup...|\n",
            "|https://www.linke...|Veterinary Recept...|\n",
            "|https://www.linke...|Optical Inspectio...|\n",
            "|https://www.linke...|HVAC, troubleshoo...|\n",
            "|https://www.linke...|Host/Server Assis...|\n",
            "|https://www.linke...|Apartment mainten...|\n",
            "|https://www.linke...|Fiber Optic Cable...|\n",
            "|https://www.linke...|CT Technologist, ...|\n",
            "|https://ca.linked...|SAP, DRMIS, Data ...|\n",
            "|https://www.linke...|Debt and equity o...|\n",
            "|https://ca.linked...|Biomedical Engine...|\n",
            "|https://www.linke...|Laboratory Techni...|\n",
            "|https://www.linke...|Program Managemen...|\n",
            "|https://www.linke...|Hiring, Training,...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df size before na dropping\n",
        "print(f\"PySpark dataset length before NA dropping: {raw_df.count()}\")\n",
        "\n",
        "# na drop\n",
        "raw_df=raw_df.na.drop()\n",
        "\n",
        "#df size after na drop\n",
        "print(f\"PySpark dataset length after NA dropping: {raw_df.count()}\")"
      ],
      "metadata": {
        "id": "_Z__4IYgEkTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef0eb90-efd3-49ec-ac8e-61f1610f9f00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark dataset length before NA dropping: 1296381\n",
            "PySpark dataset length after NA dropping: 1294374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the job_link column\n",
        "\n",
        "raw_df=raw_df.drop(\"job_link\")"
      ],
      "metadata": {
        "id": "TZWAWh5crkEI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation of the $\\mathrm{A}$-$\\mathrm{Priori}$ $\\mathrm{Algorithm}$"
      ],
      "metadata": {
        "id": "5D4N2re6Kx22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do the implementation, I will define the following function which takes as input a PySpark df and samples it in order to do all the computations.\n",
        "\n",
        "In this case, the dataset is sampled because otherwise it would be too large to analyse. So, we are going from a $\\mathrm {Cardinality}$ of the dataset, equal to $1,294,374$, to a $\\mathrm {Cardinality}$ of $13,007$ where $2\\%=260$. So, if we take a $support\\_threshold$ $s=2\\%$, we wil have that an itemset -- either a singleton, a pair, $\\dotsc$ -- will be **frequent** if it appears in at least $2\\%=260$ baskets."
      ],
      "metadata": {
        "id": "YQ5_9h7KOA_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apriori(df, s_threshold, last_frequent = 4, sampling = True, fraction = 0.01, seed = 1234):\n",
        "\n",
        "  \"\"\"\n",
        "  df -> a PySpark DataFrame object, assuming it does not contain any null or NA.\n",
        "\n",
        "  s_threshold -> the percentage value, as an integer, of the support threshold (ex. s = 1% -> s_threshold = 1, not 0.01) !!\n",
        "\n",
        "  last_frequent -> last frequent itemset to check for. If it is equal to zero it will find every single frequent itemset. If != 0, for example = 1,\n",
        "  it will only check for frequent singletons, if = 2 for pairs and so on. Default = 4.\n",
        "\n",
        "  sampling -> whether or not the df should be sampled.\n",
        "\n",
        "  fraction -> fraction of df you want to keep.\n",
        "\n",
        "  seed -> the seed of the sampling. Really important only if sampling == True.\n",
        "  \"\"\"\n",
        "  names=[\"singletons\", \"pairs\", \"triplets\", \"quadruplets\", \"pentaplets\"] # I assume that there cannot exist frequent itemsets bigger than pentaplets\n",
        "\n",
        "  finished = False\n",
        "  counter = 1\n",
        "  print(\"\"\"A-Priori Algorithm implementation\\n\\n\n",
        "Wait for a moment while the program transforms your dataset into an RDD and that finds its size.\\n\"\"\")\n",
        "\n",
        "  if sampling == True:\n",
        "    df = df.sample(False, fraction = fraction, seed = seed)\n",
        "\n",
        "  rdd = df.rdd\n",
        "  size = rdd.count()\n",
        "  support = round(size*s_threshold/100)\n",
        "\n",
        "  print(f\"Your rdd is {size} rows long and is composed by {rdd.getNumPartitions()} partitions.\\nAccording to the support, an itemset will be frequent if its count is at least {support}.\\n\")\n",
        "\n",
        "  #removing the first part of the first row\n",
        "  rdd=rdd.map(lambda bucket: {row.lower().replace(\"Row(job_skills=\",\"\") for row in bucket})\n",
        "\n",
        "  rdd=rdd.map(lambda bucket: {row.replace(\" skills\",\"\") for row in bucket})\n",
        "\n",
        "  rdd=rdd.map(lambda bucket: {row.replace(\"problemsolving\",\"problem solving\") for row in bucket})\n",
        "\n",
        "  #splitting every basket into each componing skill\n",
        "  rdd = rdd.map(lambda row: [set(skill.split(\", \")) for skill in row])\n",
        "\n",
        "  while finished==False:\n",
        "\n",
        "    print(f\"\\nSTEP {counter}\\nChecking frequent {names[counter-1]}\\nWork in progress!!\\n\")\n",
        "\n",
        "    start_Time = timeit.default_timer()\n",
        "\n",
        "    if counter == 1: #doing an if-else statement because when singletons are checked, we do not need to filter anything\n",
        "      n_pass=rdd.flatMap(lambda basket: {(skill, 1) for items in basket for skill in items})\\\n",
        "                     .reduceByKey(lambda value_x, value_y: value_x + value_y)\\\n",
        "                     .filter(lambda key_value: key_value[1]>support)\n",
        "\n",
        "\n",
        "    elif counter == 2:\n",
        "      # I called the element in the tuple \"mult_items\" = multiple items because here we are checking for pairs, triplets, ...\n",
        "      # In the first filter I am filtering those elements (the x) that are present in the frequent_items set by creating combinations of the elements in x\n",
        "\n",
        "      n_pass = rdd.flatMap(lambda basket: {(mult_items,1) for items in basket for mult_items in combinations(sorted(items), counter) \\\n",
        "                                            if all(combination in frequent_items_keys for combination in mult_items)})\\\n",
        "                   .reduceByKey(lambda value_x, value_y: value_x + value_y)\\\n",
        "                   .filter(lambda key_value: key_value[1]>support)\n",
        "\n",
        "\n",
        "    else:\n",
        "\n",
        "      n_pass = rdd.flatMap(lambda basket: {(mult_items,1) for items in basket for mult_items in combinations(sorted(items), counter) \\\n",
        "                             if all(combination in frequent_items_keys for combination in sorted(combinations(mult_items, counter-1)))})\\\n",
        "                   .reduceByKey(lambda value_x, value_y: value_x + value_y)\\\n",
        "                   .filter(lambda key_value: key_value[1]>support)\n",
        "\n",
        "\n",
        "    frequent_items = list(n_pass.collect())\n",
        "\n",
        "    frequent_items = sorted(list(frequent_items), key = lambda key_values: key_values[1], reverse = True)\n",
        "\n",
        "    frequent_items_keys = {key_values[0] for key_values in frequent_items}\n",
        "\n",
        "    end_Time = timeit.default_timer()\n",
        "    print(f\"\"\"\n",
        "Step number {counter} completed in {round(end_Time-start_Time, 2)} seconds.\n",
        "With a support theshold of {s_threshold}%, the algorithm found:\\n\n",
        "  - Number of frequent {names[counter-1]}: {len(frequent_items)}\\n\n",
        "  - Most frequent: {[(key_value[0], key_value[1]) for key_value in frequent_items[:3]]}\\n\n",
        "  - Least frequent: {[(key_value[0], key_value[1]) for key_value in frequent_items[-3::][::-1]]}\\n\n",
        "    \"\"\")\n",
        "\n",
        "    counter+=1\n",
        "\n",
        "    if len(frequent_items) == 0 or len(frequent_items) == 1 or (counter > last_frequent and last_frequent != 0) or counter > len(names):\n",
        "      finished=True\n",
        "      print(\"Project by \\033[1mGuglielmo Berzano\\033[0m, UniMi\")"
      ],
      "metadata": {
        "id": "DTt-Ma94-rZ5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apriori(raw_df, s_threshold=2, last_frequent=4, sampling=True,fraction=0.01,seed=1234)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Rm4Sww_QC8",
        "outputId": "2ae50c72-7a79-4cab-82d4-751dfbec7025"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A-Priori Algorithm implementation\n",
            "\n",
            "\n",
            "Wait for a moment while the program transforms your dataset into an RDD and that finds its size.\n",
            "\n",
            "Your rdd is 13007 rows long and is composed by 6 partitions.\n",
            "According to the support, an itemset will be frequent if its count is at least 260.\n",
            "\n",
            "\n",
            "STEP 1\n",
            "Checking frequent singletons\n",
            "Work in progress!!\n",
            "\n",
            "\n",
            "Step number 1 completed in 27.08 seconds.\n",
            "With a support theshold of 2%, the algorithm found:\n",
            "\n",
            "  - Number of frequent singletons: 77\n",
            "\n",
            "  - Most frequent: [('communication', 5618), ('problem solving', 3167), ('customer service', 2965)]\n",
            "\n",
            "  - Least frequent: [('education', 264), ('patient education', 269), ('marketing', 272)]\n",
            "\n",
            "    \n",
            "\n",
            "STEP 2\n",
            "Checking frequent pairs\n",
            "Work in progress!!\n",
            "\n",
            "\n",
            "Step number 2 completed in 30.55 seconds.\n",
            "With a support theshold of 2%, the algorithm found:\n",
            "\n",
            "  - Number of frequent pairs: 91\n",
            "\n",
            "  - Most frequent: [(('communication', 'problem solving'), 2597), (('communication', 'customer service'), 1996), (('communication', 'teamwork'), 1970)]\n",
            "\n",
            "  - Least frequent: [(('communication', 'supervision'), 261), (('leadership', 'scheduling'), 266), (('decision making', 'problem solving'), 269)]\n",
            "\n",
            "    \n",
            "\n",
            "STEP 3\n",
            "Checking frequent triplets\n",
            "Work in progress!!\n",
            "\n",
            "\n",
            "Step number 3 completed in 117.28 seconds.\n",
            "With a support theshold of 2%, the algorithm found:\n",
            "\n",
            "  - Number of frequent triplets: 51\n",
            "\n",
            "  - Most frequent: [(('communication', 'problem solving', 'teamwork'), 1271), (('communication', 'customer service', 'problem solving'), 1076), (('communication', 'leadership', 'problem solving'), 935)]\n",
            "\n",
            "  - Least frequent: [(('collaboration', 'communication', 'leadership'), 263), (('communication', 'leadership', 'training'), 264), (('communication', 'nursing', 'patient care'), 269)]\n",
            "\n",
            "    \n",
            "\n",
            "STEP 4\n",
            "Checking frequent quadruplets\n",
            "Work in progress!!\n",
            "\n",
            "\n",
            "Step number 4 completed in 1480.09 seconds.\n",
            "With a support theshold of 2%, the algorithm found:\n",
            "\n",
            "  - Number of frequent quadruplets: 16\n",
            "\n",
            "  - Most frequent: [(('communication', 'customer service', 'problem solving', 'teamwork'), 587), (('communication', 'problem solving', 'teamwork', 'time management'), 516), (('communication', 'leadership', 'problem solving', 'teamwork'), 509)]\n",
            "\n",
            "  - Least frequent: [(('customer service', 'leadership', 'problem solving', 'teamwork'), 261), (('attention to detail', 'problem solving', 'teamwork', 'time management'), 268), (('attention to detail', 'communication', 'teamwork', 'time management'), 276)]\n",
            "\n",
            "    \n",
            "Project by \u001b[1mGuglielmo Berzano\u001b[0m, UniMi\n"
          ]
        }
      ]
    }
  ]
}
